{
  "title": "Kinect Sensor",
  "description": "Notes about body tracking for interactive installations and filmaking.",
  "date": "2014-09-16T00:00:00.000Z",
  "thumbnail": "/static/lab/quale/kinect-danisho.jpg",
  "published": true,
  "body": {
    "raw": "\n![Danisho](/static/lab/quale/kinect-danisho.jpg)\n\n{/* ![Kinect sensor v1](https://github.com/colormono/kinect-notes/raw/master/assets/images/kinectp-640x353.jpg) */}\n\nThe Kinect sensor is a motion-sensing input device developed by Microsoft for use with Xbox gaming consoles and later adapted for other applications. It utilizes a combination of depth sensing, RGB imaging, and skeletal tracking to capture and interpret human movement and gestures.\n\n![Kinect sensor v1 interior](https://github.com/colormono/kinect-notes/raw/master/assets/images/kinect-interior.png)\n\nThe Kinect sensor consists of a camera, an infrared emitter, and an array of sensors that work together to create a detailed 3D map of the surrounding environment. It emits an infrared pattern and measures the time it takes for the pattern to bounce back to the sensor, allowing it to calculate depth information.\n\n![Kinect sensor v1 joints](https://github.com/colormono/kinect-notes/raw/master/assets/images/synapse.jpg)\n\nUsing this depth data and the RGB camera, the Kinect sensor can track the position and movement of multiple individuals within its field of view. It can recognize and track skeletal joints, such as the hands, head, and limbs, enabling more accurate and nuanced tracking of human gestures and actions.\n\nOriginally designed for gaming, the Kinect sensor gained popularity beyond gaming applications. It has been used in various fields such as robotics, healthcare, virtual reality, and interactive installations. The sensor's ability to capture human motion in real-time has made it a valuable tool for creating immersive experiences, interactive interfaces, and innovative applications across different industries.\n\n![Kinect sensor v1 ok](https://github.com/colormono/kinect-notes/raw/master/assets/images/ofxKinect.jpg)\n\n### V1\n\n- It was released in late 2010.\n- The capture depth ranges from 0.5 to 4.5 meters.\n- 30 FPS with a resolution of 640x480 for RGB, and 320x240 for Depth (D).\n- Compatible models: 1414 y 1473 (buggy).\n- Needs a PC adapter.\n- Windows, OSx and Linux support.\n- USB 2.0 connection.\n\n### V2\n\n- It was released in late 2013.\n- The capture depth remains the same.\n- It sends data at 30 FPS with a resolution of 1920x1080 for RGB and 512x424 for Depth (D).\n- It includes a \"Registered\" image that aligns the RGB camera with the Depth (D) camera.\n- It requires a very specific adapter (no generic adapters available).\n- It can be used with Windows (>=8, 64 bit) and OSx (>= 10.9; without image analysis).\n- USB 3.0 connection.\n\n## Resources\n\n- [Workshop Notes and Code repository](https://github.com/colormono/kinect-notes). UNA, 2017\n\n### Related works\n\n- [Locomoción](https://colormono.com/works/locomocion): Interactive installation\n- [Música, Tan Bionica](https://colormono.com/works/musica): RGBD Filmmaking\n",
    "code": "var Component=(()=>{var d=Object.create;var s=Object.defineProperty;var m=Object.getOwnPropertyDescriptor;var p=Object.getOwnPropertyNames;var u=Object.getPrototypeOf,g=Object.prototype.hasOwnProperty;var f=(i,e)=>()=>(e||i((e={exports:{}}).exports,e),e.exports),b=(i,e)=>{for(var a in e)s(i,a,{get:e[a],enumerable:!0})},o=(i,e,a,r)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let t of p(e))!g.call(i,t)&&t!==a&&s(i,t,{get:()=>e[t],enumerable:!(r=m(e,t))||r.enumerable});return i};var k=(i,e,a)=>(a=i!=null?d(u(i)):{},o(e||!i||!i.__esModule?s(a,\"default\",{value:i,enumerable:!0}):a,i)),v=i=>o(s({},\"__esModule\",{value:!0}),i);var l=f((N,c)=>{c.exports=_jsx_runtime});var j={};b(j,{default:()=>y,frontmatter:()=>w});var n=k(l());var w={title:\"Kinect Sensor\",description:\"Notes about body tracking for interactive installations and filmaking.\",thumbnail:\"/static/lab/quale/kinect-danisho.jpg\",date:new Date(14108256e5)};function h(i){let e=Object.assign({p:\"p\",img:\"img\",h3:\"h3\",a:\"a\",span:\"span\",ul:\"ul\",li:\"li\",h2:\"h2\"},i.components);return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/static/lab/quale/kinect-danisho.jpg\",alt:\"Danisho\"})}),`\n`,`\n`,(0,n.jsx)(e.p,{children:\"The Kinect sensor is a motion-sensing input device developed by Microsoft for use with Xbox gaming consoles and later adapted for other applications. It utilizes a combination of depth sensing, RGB imaging, and skeletal tracking to capture and interpret human movement and gestures.\"}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"https://github.com/colormono/kinect-notes/raw/master/assets/images/kinect-interior.png\",alt:\"Kinect sensor v1 interior\"})}),`\n`,(0,n.jsx)(e.p,{children:\"The Kinect sensor consists of a camera, an infrared emitter, and an array of sensors that work together to create a detailed 3D map of the surrounding environment. It emits an infrared pattern and measures the time it takes for the pattern to bounce back to the sensor, allowing it to calculate depth information.\"}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"https://github.com/colormono/kinect-notes/raw/master/assets/images/synapse.jpg\",alt:\"Kinect sensor v1 joints\"})}),`\n`,(0,n.jsx)(e.p,{children:\"Using this depth data and the RGB camera, the Kinect sensor can track the position and movement of multiple individuals within its field of view. It can recognize and track skeletal joints, such as the hands, head, and limbs, enabling more accurate and nuanced tracking of human gestures and actions.\"}),`\n`,(0,n.jsx)(e.p,{children:\"Originally designed for gaming, the Kinect sensor gained popularity beyond gaming applications. It has been used in various fields such as robotics, healthcare, virtual reality, and interactive installations. The sensor's ability to capture human motion in real-time has made it a valuable tool for creating immersive experiences, interactive interfaces, and innovative applications across different industries.\"}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"https://github.com/colormono/kinect-notes/raw/master/assets/images/ofxKinect.jpg\",alt:\"Kinect sensor v1 ok\"})}),`\n`,(0,n.jsxs)(e.h3,{id:\"v1\",children:[(0,n.jsx)(e.a,{className:\"subheading-anchor\",\"aria-label\":\"Link to section\",href:\"#v1\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"V1\"]}),`\n`,(0,n.jsxs)(e.ul,{children:[`\n`,(0,n.jsx)(e.li,{children:\"It was released in late 2010.\"}),`\n`,(0,n.jsx)(e.li,{children:\"The capture depth ranges from 0.5 to 4.5 meters.\"}),`\n`,(0,n.jsx)(e.li,{children:\"30 FPS with a resolution of 640x480 for RGB, and 320x240 for Depth (D).\"}),`\n`,(0,n.jsx)(e.li,{children:\"Compatible models: 1414 y 1473 (buggy).\"}),`\n`,(0,n.jsx)(e.li,{children:\"Needs a PC adapter.\"}),`\n`,(0,n.jsx)(e.li,{children:\"Windows, OSx and Linux support.\"}),`\n`,(0,n.jsx)(e.li,{children:\"USB 2.0 connection.\"}),`\n`]}),`\n`,(0,n.jsxs)(e.h3,{id:\"v2\",children:[(0,n.jsx)(e.a,{className:\"subheading-anchor\",\"aria-label\":\"Link to section\",href:\"#v2\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"V2\"]}),`\n`,(0,n.jsxs)(e.ul,{children:[`\n`,(0,n.jsx)(e.li,{children:\"It was released in late 2013.\"}),`\n`,(0,n.jsx)(e.li,{children:\"The capture depth remains the same.\"}),`\n`,(0,n.jsx)(e.li,{children:\"It sends data at 30 FPS with a resolution of 1920x1080 for RGB and 512x424 for Depth (D).\"}),`\n`,(0,n.jsx)(e.li,{children:'It includes a \"Registered\" image that aligns the RGB camera with the Depth (D) camera.'}),`\n`,(0,n.jsx)(e.li,{children:\"It requires a very specific adapter (no generic adapters available).\"}),`\n`,(0,n.jsx)(e.li,{children:\"It can be used with Windows (>=8, 64 bit) and OSx (>= 10.9; without image analysis).\"}),`\n`,(0,n.jsx)(e.li,{children:\"USB 3.0 connection.\"}),`\n`]}),`\n`,(0,n.jsxs)(e.h2,{id:\"resources\",children:[(0,n.jsx)(e.a,{className:\"subheading-anchor\",\"aria-label\":\"Link to section\",href:\"#resources\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Resources\"]}),`\n`,(0,n.jsxs)(e.ul,{children:[`\n`,(0,n.jsxs)(e.li,{children:[(0,n.jsx)(e.a,{href:\"https://github.com/colormono/kinect-notes\",children:\"Workshop Notes and Code repository\"}),\". UNA, 2017\"]}),`\n`]}),`\n`,(0,n.jsxs)(e.h3,{id:\"related-works\",children:[(0,n.jsx)(e.a,{className:\"subheading-anchor\",\"aria-label\":\"Link to section\",href:\"#related-works\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Related works\"]}),`\n`,(0,n.jsxs)(e.ul,{children:[`\n`,(0,n.jsxs)(e.li,{children:[(0,n.jsx)(e.a,{href:\"https://colormono.com/works/locomocion\",children:\"Locomoci\\xF3n\"}),\": Interactive installation\"]}),`\n`,(0,n.jsxs)(e.li,{children:[(0,n.jsx)(e.a,{href:\"https://colormono.com/works/musica\",children:\"M\\xFAsica, Tan Bionica\"}),\": RGBD Filmmaking\"]}),`\n`]})]})}function x(i={}){let{wrapper:e}=i.components||{};return e?(0,n.jsx)(e,Object.assign({},i,{children:(0,n.jsx)(h,i)})):h(i)}var y=x;return v(j);})();\n/*![Kinect sensor v1](https://github.com/colormono/kinect-notes/raw/master/assets/images/kinectp-640x353.jpg)*/\n;return Component;"
  },
  "_id": "posts/kinect.mdx",
  "_raw": {
    "sourceFilePath": "posts/kinect.mdx",
    "sourceFileName": "kinect.mdx",
    "sourceFileDir": "posts",
    "contentType": "mdx",
    "flattenedPath": "posts/kinect"
  },
  "type": "Post",
  "slug": "/posts/kinect",
  "slugAsParams": "kinect"
}